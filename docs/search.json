[
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "",
    "text": "Are you bidding right on your marketing campaigns?"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#modelling-digital-advertisement",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#modelling-digital-advertisement",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "1- Modelling digital advertisement",
    "text": "1- Modelling digital advertisement\nThe digital advertisement is rich with methods on how a monetary quantity (a bid, a bid cap, a budget, etc.) is connected to how many users - or leads, conversions, installs, subscribers - an advertiser acquires. For simplicity and without loss of generalization, we define for the rest of the document that we are reaching users for a digital product and use Cost per User as the lever that controls the volume (i.e., the quantity of the desired unit) acquired through advertisement, which can be easily exchanged to a bid or ROAS (Revenue over Ad Spend).\nIn a digital advertisement, the ultimate goal is to obtain a profit from the marketing campaign, defined as\n\\[ Profit \\triangleq Revenue - Costs\\]\n, where \\(Revenue\\) and \\(Cost\\) can be further broken down:\n\\[ Profit = Users * (LTV_{user} - Cost_{user})\\]\n, where \\(Users\\) stands for the number of users acquired from the marketing campaign, and \\(LTV_{user}\\) and \\(Cost_{user}\\) for the average LTV and Cost of those users, respectively.\nBut the number of users acquired from a marketing campaign depends on the digital platform, the advertised product, and how many we will put for a user. We are going to call this relationship Volume Function \\(f\\left(Cost_{user}\\right)\\):\n\\[ Installs \\triangleq f\\left(Cost_{user}\\right)\\]\nThe resulting \\(Profit\\) function thus becomes:\n\\[ Profit = f\\left(Cost_{user}\\right) * (LTV_{user} - Cost_{user})\\]\nAnd if the volume function and \\(LTV_{user}\\) are known, the \\(Cost_{user}\\) that delivers the highest \\(Profit\\) can be found using an optimization function as [scipy.optimize.minimize] in Python. However, as we will show, it is less obvious when \\(LTV_{user}\\) is uncertain."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-volume-function-how-the-cost-per-user-defines-the-acquired-number-of-users",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-volume-function-how-the-cost-per-user-defines-the-acquired-number-of-users",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "2- The Volume Function: how the Cost per User defines the acquired number of users",
    "text": "2- The Volume Function: how the Cost per User defines the acquired number of users\nIn the previous section, we introduced the Volume Function \\(f(x)\\) but didnâ€™t go into much detail. As this function defines the association between Cost per User and the number of users from a marketing campaign, it has to satisfy the following conditions:\n\n\\(f(x) = 0\\), \\(\\forall x \\leq 0\\),\n\nThis is quite simple: an advertising platform will only show an ad to its user if it gains\n\n\\(f(x)\\) is strictly monotonically non-decreasing\n\nWhen we advertise on a platform, we first reach users not highly valued by other advertisers or who have a good match with our product. All else constant, the only way to get new customers is by paying a higher price for those additional users to win against the current winners. As a consequence, the average Cost per User increases.\n\n\\(\\lim_{x \\to \\infty} = D\\)\n\nThis condition states that a marketing platform has a maximum number of people we can reach, regardless of size. In the case of Meta and Google, this limit can be in the order of billions.\n\n\nWhile many functions satisfy the requirements above, a simple one is the cumulative probability function of a log-normal distribution multiplied by a constant:\n\\[f\\left(Cost_{user}\\right)_{Log-Norm Distribution} = User_{Limit}*CDF_{log-norm}(Cost_{user}, \\mu, \\sigma)\\]\nwhere \\(CDF_{Log-Norm Distribution}\\) refers to the cumulative probability function of a log-normal distribution. While $ User_{limit}$ is unknown to the advertiser, it can be estimated from previous marketing campaigns:\n\\[Install_{Ref} =  User_{Limit}*CDF_{log-norm}(Cost_{user, reference}, \\mu, \\sigma)\\]\n\\[User_{Limit} = \\frac{Install_{Ref}}{CDF_{log-norm}(Cost_{user, reference}, \\mu, \\sigma)}\\]\nWe will show for different log-normal distributions how the volume should vary per \\(Cost_{user}\\), and, consequently, the profit curve and the optimal bidding point. For that, we fix the following variables.\n\\[Cost_{user, reference} = 2\\] \\[E[LTV] = 2\\] \\[Install_{Ref} = 1000\\]\nIn addition, we will consider four different log-normal distributions for the Volume Function, with expected values of 0.5, 1.0, 1.5, and 2 and a standard deviation of 0.5 (for the normal distribution).\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom joblib import Parallel, delayed\nfrom typing import List\n\nfrom src.VolumeAcquisition import CumulativeLognormalVolume\n\nsns.set_style('whitegrid')\nrng = np.random.default_rng(42)\nPLOT_WITHD_INCHES = 20\nPLOT_HEIGHT_INCHES = 10\n# The fixed parameters specified before\nreference_volume = 1000\nreference_cpi = 2.0\nreference_ltv = 2.0\n\nlognormal_exp_values = [0.5, 1, 1.5, 2.0] # the expected values for the \ncpi_range = np.linspace(0.001, 6.5, 1000) # we want to consider cost per user between $0 and $6.5\ndef generate_marketing_properties(\n    cpi_range: list, \n    lognormal_expected_values: List[float],\n    reference_volume: int,\n    reference_cpi: float,\n    reference_ltv: float\n):\n    \"\"\"\n    This class calculates (volume, revenue, profit, cost) for each CPI value in [cpi_range].\n    The volume curve changes depending on the value from [volume_params], but it always gives [reference_volume] for [reference_cpi]\n    In addition to the marketing properties, it also provides estimates on the CDF and PDF of the assumed distribution of volume by CPI\n    \"\"\"\n    output_data = []\n    for exp_value in lognormal_expected_values:\n        # define how the volume behaves\n        volume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, average=exp_value, standard_deviation=0.5)\n        \n        # calculate volume, and cdf for diferent cpis\n        volume = volume_model.calculate_volume(cpi_range)\n        cdf = volume_model.calculate_cdf(cpi_range)\n        pdf = volume_model.calculate_pdf(cpi_range)\n        df = pd.DataFrame({'cpi': cpi_range, 'volume': volume, 'cdf': cdf, 'pdf': pdf})\n        \n        # store which lognormal expected value was used\n        df['expected_value'] = exp_value\n        output_data.append(df)\n\n    # merge the data from the different volume functions together\n    output_data = pd.concat(output_data)\n    \n    # calculate the profit for each cpi\n    output_data['profit'] = output_data['volume'] * (reference_ltv - output_data['cpi'])\n\n    # Cast the avg to make the seaborn interpret the values as category\n    output_data['expected_value'] = output_data['expected_value'].astype(str)\n    return output_data\n# generate the volume and profit curves for our scenarios and store them in a pd.DataFrame\nlog_norm_volume_data = generate_marketing_properties(\n    cpi_range,\n    lognormal_exp_values,\n    reference_volume,\n    reference_cpi,\n    reference_ltv\n    \n)\nBelow you can see how the density probability functions with the selected parameters looks like\ngrid = sns.relplot(log_norm_volume_data, x='cpi', y='pdf', hue='expected_value', kind='line')\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Probability density function (Y) versus cost per user (X), versus mean of the log-normal distribution', \n    font='Avenir', \n    fontsize=24, \n    loc='left'\n)\nText(0.0, 1.0, 'Probability density function (Y) versus cost per user (X), versus mean of the log-normal distribution')\n\nlog_norm_volume_data = log_norm_volume_data[log_norm_volume_data['cpi']< 2.5]\ngrid = sns.relplot(log_norm_volume_data, x='cpi', y='volume', hue='expected_value', kind='line')\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Number of installs (Y) versus cost per install (X), versus mean of the log-normal distribution', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Number of installs (Y) versus cost per install (X), versus mean of the log-normal distribution')\n\nEach curve can represent different advertisement platforms: the blue curve could be a small platform where most users can already be acquired with a relatively low cost of $4, while the red curve can represent a platform with many users and a lot of competition from other advertisers since there are a lot of users we can acquire - as the curve quickly increases after $2."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-profit-curve-how-the-expected-profit-varies-per-cost-per-user",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-profit-curve-how-the-expected-profit-varies-per-cost-per-user",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "3- The Profit Curve: how the expected profit varies per Cost per User",
    "text": "3- The Profit Curve: how the expected profit varies per Cost per User\nBy using these curves in the \\(Profit\\) function, we get the following profit curve by cost per user.\ngrid = sns.relplot(log_norm_volume_data, x='cpi', y='profit', hue='expected_value', kind='line')\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Predicted expected profit (Y) versus cost per install (X), versus mean of the log-normal distribution', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Predicted expected profit (Y) versus cost per install (X), versus mean of the log-normal distribution')\n\nWhich all have slightly different optimal \\(Cost_{user}\\)\nlog_norm_volume_data.sort_values(['expected_value', 'profit']).groupby(['expected_value'])['cpi'].last().reset_index()\n\n\n\n\n\n\n\n\n\nexpected_value\n\n\ncpi\n\n\n\n\n\n\n0\n\n\n0.5\n\n\n1.360651\n\n\n\n\n1\n\n\n1.0\n\n\n1.536299\n\n\n\n\n2\n\n\n1.5\n\n\n1.653398\n\n\n\n\n3\n\n\n2.0\n\n\n1.731464\n\n\n\n\n\n\nWhile the highest profit isnâ€™t relevant when comparing the curves, since we forced all the curves to provide the same number of users at $2, the â€˜shapeâ€™ is. Notice how the profit curve drawn above is asymmetric around the optimal \\(Cost_{user}\\) for most curves, as the profit quickly drops as the \\(Cost_{user}\\) increases from its optimal point. This asymmetry will be essential to understand why and when the uncertainty on LTV matters."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-error-associated-with-the-predicted-ltv",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-error-associated-with-the-predicted-ltv",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "4- The error associated with the predicted LTV",
    "text": "4- The error associated with the predicted LTV\nWith the clear connection between Cost per User and profit, we can focus on LTV. The Lifetime Value of a user is, by definition, how much revenue a user generates in their lifetime, in other words, from the moment they started their â€˜lifeâ€™ as a user of a product until infinity. But we canâ€™t use infinity in practice, in good part because we need a target to train the machine-learning models, so the lifetime value is usually defined as just a date â€˜sufficiently awayâ€™ in the future.\nThe fact that the â€˜realâ€™ LTV is usually too far away in the future means that it cannot be used for practically any critical decision within a company. This means that the LTV used for such decisions will be an estimate, and as an estimate, we are sure of its exact value.\nIf we estimate the LTV of our users using any conventional machine-learning algorithm, one assumption that these models require is for the residual (basically the error) to be normally distributed. As such, we will assume that the LTV modelsâ€™ error is (1) unbiased and (2) follow a normal distribution.\n\\[LTV \\sim N(\\mu, \\sigma) \\]\n\\[LTV_{predicted} = LTV + \\epsilon\\]\n\\[\\epsilon \\sim N(0, \\delta)\\]\n, where \\(\\epsilon\\) is the residual and \\(\\delta\\) is its standard devitiation.\nrng = np.random.default_rng(42)\nltv_error_std = 0.5\nnorm_dist = rng.normal(0, ltv_error_std, 10000)\n\ngrid = sns.histplot(norm_dist)\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Distribution of the error of the LTV predictions', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Distribution of the error of the LTV predictions')\n\nAgain, I will reinforce that LTV is a predicted metric, and as such until proven otherwise, it is what we believe to be true. That is the case for any metric, but for most metrics, ML models predict, it becomes clear relatively fast if the model was wrong or not. But it can take many months until signs appear of bias in the LTV predictions, which can mean several thousand, if not tens or hundreds of thousands of dollars, invested in marketing that are not coming back\nFor example, assume five different scenarios where we keep the actual LTV at 2 dollars and use the Volume Function in orange from the first plot but have the estimated LTV to be $1, $1.5, $2, $2.5, or Â $3. If we know the Volume Function and we were to optimize for the predicted expected value of LTV without budget being a constrain, we would operate at the indicated estimated optimal cpi, believe we would obtain the estimated profit while in practice it would be the real profit\npossible_ltvs = [1, 1.5, 2, 2.5, 3, 3.5]\nfixed_cpi = [1]\nestimated_optimal_cpi = []\nfor ltv in possible_ltvs:\n    marketing_data = generate_marketing_properties(\n        cpi_range,\n        fixed_cpi,\n        reference_volume,\n        reference_cpi,\n        ltv\n    )\n    estimated_optimal_cpi.append(list(marketing_data.sort_values(['profit'])['cpi'])[-1])\n\nestimated_optimal_cpi = pd.DataFrame({'cpi': estimated_optimal_cpi, 'estimated_ltv': possible_ltvs})\n\nestimated_optimal_cpi = pd.merge(\n    log_norm_volume_data[log_norm_volume_data['expected_value'] == '1.0'],\n    estimated_optimal_cpi,\n    on='cpi'\n)\nestimated_optimal_cpi['estimated profit'] = (estimated_optimal_cpi['estimated_ltv'] - estimated_optimal_cpi['cpi'])*estimated_optimal_cpi['volume']\nestimated_optimal_cpi['real profit'] = estimated_optimal_cpi['profit']\n\n\n\nestimated_ltv\ncpi\nvolume\nestimated profit\nreal profit\n\n\n\n\n1.0\n0.840210\n34.971618\n5.588107\n40.559725\n\n\n1.5\n1.204519\n191.985179\n56.728065\n152.720654\n\n\n2.0\n1.536299\n470.453096\n218.149430\n218.149430\n\n\n2.5\n1.829047\n793.685368\n532.525541\n135.682857\n\n\n3.0\n2.095773\n1117.805605\n1010.750263\n-107.055342\n\n\n3.5\n2.336476\n1412.857895\n1643.893396\n-475.393446\n\n\n\nNotice the discrepancy between the estimated and actual profits when we overestimate LTV. While this discrepancy is minor when the predicted LTV is underestimated (and usually, getting more money than expected isnâ€™t badly received news), the disparity quickly increases when the predictions are overestimated.\nThis behavior can be better understood when we plot the profit distribution when the standard deviation is 1 (i.e., 50% of the actual LTV). As shown below, while the profit is mostly positive, we have a situation where it is negative, as demonstrated by the long left tail.\n# See how the distribution of the profit is, when we are wrong about the LTV\nfrom src.ProfitModel import StandardModel\nvolume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, average=1, standard_deviation=0.5)\nprofit_model = StandardModel(volume_model, lifetime_value=2)\n\n# calculate the profit\ncpis = rng.normal(reference_ltv, reference_ltv*ltv_error_std, 10000)\nprofit_distribution = profit_model.calculate_profit(cpis)\n\ngrid = sns.histplot(profit_distribution)\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    f'Distribution of the profit when the LTV error is normally distributed with average 0 and standard deviation equal to {ltv_error_std*reference_ltv}', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Distribution of the profit when the LTV error is normally distributed with average 0 and standard deviation equal to 1.0')\n\nWhile one may think that a 50% error for a prediction is too high, that is quite common for a new product being launched, for a marketing platform being introduced, or for a novel optimization within an existing platform. I.E. for any case where there may not be much data available and the situation is sufficiently novel from what was seen before. So given the risk involved in incorrectly predicting and specially overestimating the LTV, how should we adapt the bidding strategy change for different degrees of uncertainty?"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#step-by-step-explanation-of-the-simulation",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#step-by-step-explanation-of-the-simulation",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "5.1- Step-by-step explanation of the simulation",
    "text": "5.1- Step-by-step explanation of the simulation\nWe set up the parameters for the simulation\n\nsample_size defines how many simulations we are doing. In the code it is called sample size because, effectively, each simulation is sampling a value for the LTV error from a normal distribution. In this step we are going to do 1 sample.\nstandard_deviation_of_ltv_error defines the range considered for the simulation for the standard error of LTV. This value is relative to the expected LTV.\nfraction_of_estimated_ltv defines the fraction of the predicted LTV we use to bid. So if we predicted a LTV of $1.0 and the selected fraction is 0.8, we will re-scale the LTV to be $0.80 and find the optimal Cost per User under this scenario. While we state it is a fraction, we include values greater than 1 just to demonstrate that it is never better to â€˜overpayâ€™ for the Volume Functions used throughout this document\n\nfrom src.BidOptimizer import StandardBidOptimizer\n\nsample_size = 1\nstandard_deviation_of_ltv_error = [0.4]\nfraction_of_estimated_ltv = [0.6, 0.8]\navg = 2\nWe then define the volume model with the previously mentioned parameters:\nvolume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, avg, .5)\nAnd now we create the bid optimizer object bid_optim. This class has 2 main methods:\n\nsimulate(): This method is going to extract samples from the normal distribution, then for each of the selected fractions we multiply the predicted LTV to obtain the adjusted LTV, which then is used as the Cost per User. Finally, the Cost per User is used to calculate the profit.\ncalculate_bidding_strategy_results(): This method uses the previous output, and calculates for each LTV standard error and fraction, the average profit. Then, for each standard error, it finds which LTV fraction provided the highest average profit\n\nbid_optim = StandardBidOptimizer(\n    volume_model, \n    standard_deviation_of_ltv_error, \n    fraction_of_estimated_ltv, \n    reference_ltv_value=2.0,\n    sample_size=sample_size)\n# simulate the 2 scenarions (because we set sample_size to 2)\nsim_results = bid_optim.simulate()\n\n# see the output from the simulation:\nsim_results\n\n\n\n\n\n\n\n\n\nestimated_ltv\n\n\nltv_fraction\n\n\ncpi\n\n\nprofit\n\n\nsd\n\n\n\n\n\n\n0\n\n\n2.121887\n\n\n0.6\n\n\n1.131382\n\n\n16.937414\n\n\n0.4\n\n\n\n\n1\n\n\n2.121887\n\n\n0.8\n\n\n1.484203\n\n\n76.374987\n\n\n0.4\n\n\n\n\n2\n\n\n1.584006\n\n\n0.6\n\n\n0.855907\n\n\n2.074194\n\n\n0.4\n\n\n\n\n3\n\n\n1.584006\n\n\n0.8\n\n\n1.126308\n\n\n16.435281\n\n\n0.4\n\n\n\n\n\n\nYou can see that we have 2 rows for each estimated_ltv as a consequence of the 2 different ltv_fraction selected, resulting in the predicted optimal Cost per User cpi. This cpi is then used to acquire users and results in the actual profit you see there. Remeber that for all simulations the actual LTV is held at $2.0\nWe now calculate which of the LTV fractions gave the highest average profit:\nbid_optim.calculate_bidding_strategy_results(sim_results)\nbid_optim.bidding_strategy_data\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n\n\n\n\n\n\n\n\n\nsd\n\n\nmean_profit\n\n\nstd_profit\n\n\nltv_fraction\n\n\n\n\n\n\n0\n\n\n0.4\n\n\n46.405134\n\n\n29.969853\n\n\n0.8\n\n\n\n\n\n\nWhich in this case was 0.8, at $72.36"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#simulation",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#simulation",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "5.2- Simulation",
    "text": "5.2- Simulation\nNow that it is clear what is happening, we can find for each of the Volume Functions from before and for different levels of LTV standard error, which fraction gives the highest average profit\nsample_size = 5000\nstandard_deviation_of_ltv_error = np.linspace(0.001, 0.4, 10)\nfraction_of_estimated_ltv = np.linspace(0.3, 1.2, 20)\noutput_data = []\n# for each distribution of the Volume Function\nfor avg in lognormal_exp_values:\n    # set up the volume model\n    volume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, avg, .5)\n    \n    # set up the class that will sample the standard error, and calculate the resulting bid and profit\n    bid_optim = StandardBidOptimizer(\n        volume_model, \n        standard_deviation_of_ltv_error, \n        fraction_of_estimated_ltv, \n        sample_size=sample_size)\n    \n    # run the N=sample_size simulations\n    sim_results = bid_optim.simulate()\n    \n    # calculate which LTV fraction gave the highest average profit for each LTV standard error level\n    bid_optim.calculate_bidding_strategy_results(sim_results)\n    \n    # store the results\n    bid_optim.bidding_strategy_data['lognormal_avg'] = avg\n    output_data.append(bid_optim.bidding_strategy_data)\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\nplot_data = pd.concat(output_data)\nplot_data['lognormal_avg'] = plot_data['lognormal_avg'].astype(str)\n\ngrid = sns.relplot(plot_data, \n                   x='sd', \n                   y='ltv_fraction', \n                   hue='lognormal_avg', \n                   kind='line', \n                   facet_kws={'ylim': [0, 1.1]}\n                  )\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Fraction of Cost per User with highest average profit when there is not uncertainty (Y) versus error level of LTV (X), versus mean for log-normal distribution',\n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Fraction of Cost per User with highest average profit when there is not uncertainty (Y) versus error level of LTV (X), versus mean for log-normal distribution')\n\ngrid = sns.relplot(plot_data, \n                   x='sd', \n                   y='mean_profit', \n                   hue='lognormal_avg', \n                   kind='line',\n                   facet_kws={'ylim': [0, 400]}\n                  )\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Average profit from the optimal LTV fraction strategy (Y) versus error level of LTV (X), versus mean for log-normal distribution',\n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Average profit from the optimal LTV fraction strategy (Y) versus error level of LTV (X), versus mean for log-normal distribution')\n\nAs expected, the greater the uncertainty of LTV, the more â€˜conservativeâ€™ we should be on our marketing strategy. But that doesnâ€™t only depend on the LTV uncertainty itself but also on the Volume Function. For Volume Functions where the profit curve is highly asymmetric around the optimal Cost per User, such as when \\(lognormal_{avg}\\) is 2 (red), even for a slight standard error of 0.1 we already have to bid less than the expected optimal point. However, we donâ€™t need to be as conservative when the Profit Curve is more symmetric (i.e.Â the volume doesnâ€™t increase as fast).\nNotice also how the profit curve decreases similarly for all Volume Functions from where LTV error is close to 0 to where it is close to 0.4. However, the relative profit loss is much higher when the volume increase fast. While the loss for \\(lognormal_{avg}\\) equal to 0.5 (blue) from the two extremes is less than 9%, for \\(lognormal_{avg}\\) equal to 2 (red) the loss is 48%. That is a result from the need to bid further away from the optimal point and from the higher losses, when we overbid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Raphael Tamaki's Blog",
    "section": "",
    "text": "lifetime value\n\n\nmarketing\n\n\ndigital marketing\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nRaphael de Brito Tamaki\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my personal blog, where I share some thoughts in topics I have experience or interest in. Feel free to reach out to me in LinkedIn or on my email"
  }
]