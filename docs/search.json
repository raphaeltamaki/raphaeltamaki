[
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "",
    "text": "Are you bidding right on your marketing campaigns?"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#modelling-digital-advertisement",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#modelling-digital-advertisement",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "1- Modelling digital advertisement",
    "text": "1- Modelling digital advertisement\nThe digital advertisement is rich with methods on how a monetary quantity (a bid, a bid cap, a budget, etc.) is connected to how many users - or leads, conversions, installs, subscribers - an advertiser acquires. For simplicity and without loss of generalization, we define for the rest of the document that we are reaching users for a digital product and use Cost per User as the lever that controls the volume (i.e., the quantity of the desired unit) acquired through advertisement, which can be easily exchanged to a bid or ROAS (Revenue over Ad Spend).\nIn a digital advertisement, the ultimate goal is to obtain a profit from the marketing campaign, defined as\n\\[ Profit \\triangleq Revenue - Costs\\]\n, where \\(Revenue\\) and \\(Cost\\) can be further broken down:\n\\[ Profit = Users * (LTV_{user} - Cost_{user})\\]\n, where \\(Users\\) stands for the number of users acquired from the marketing campaign, and \\(LTV_{user}\\) and \\(Cost_{user}\\) for the average LTV and Cost of those users, respectively.\nBut the number of users acquired from a marketing campaign depends on the digital platform, the advertised product, and how many we will put for a user. We are going to call this relationship Volume Function \\(f\\left(Cost_{user}\\right)\\):\n\\[ Installs \\triangleq f\\left(Cost_{user}\\right)\\]\nThe resulting \\(Profit\\) function thus becomes:\n\\[ Profit = f\\left(Cost_{user}\\right) * (LTV_{user} - Cost_{user})\\]\nAnd if the volume function and \\(LTV_{user}\\) are known, the \\(Cost_{user}\\) that delivers the highest \\(Profit\\) can be found using an optimization function as [scipy.optimize.minimize] in Python. However, as we will show, it is less obvious when \\(LTV_{user}\\) is uncertain."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-volume-function-how-the-cost-per-user-defines-the-acquired-number-of-users",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-volume-function-how-the-cost-per-user-defines-the-acquired-number-of-users",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "2- The Volume Function: how the Cost per User defines the acquired number of users",
    "text": "2- The Volume Function: how the Cost per User defines the acquired number of users\nIn the previous section, we introduced the Volume Function \\(f(x)\\) but didn’t go into much detail. As this function defines the association between Cost per User and the number of users from a marketing campaign, it has to satisfy the following conditions:\n\n\\(f(x) = 0\\), \\(\\forall x \\leq 0\\),\n\nThis is quite simple: an advertising platform will only show an ad to its user if it gains\n\n\\(f(x)\\) is strictly monotonically non-decreasing\n\nWhen we advertise on a platform, we first reach users not highly valued by other advertisers or who have a good match with our product. All else constant, the only way to get new customers is by paying a higher price for those additional users to win against the current winners. As a consequence, the average Cost per User increases.\n\n\\(\\lim_{x \\to \\infty} = D\\)\n\nThis condition states that a marketing platform has a maximum number of people we can reach, regardless of size. In the case of Meta and Google, this limit can be in the order of billions.\n\n\nWhile many functions satisfy the requirements above, a simple one is the cumulative probability function of a log-normal distribution multiplied by a constant:\n\\[f\\left(Cost_{user}\\right)_{Log-Norm Distribution} = User_{Limit}*CDF_{log-norm}(Cost_{user}, \\mu, \\sigma)\\]\nwhere \\(CDF_{Log-Norm Distribution}\\) refers to the cumulative probability function of a log-normal distribution. While \\(User_{limit}\\) is unknown to the advertiser, it can be estimated from previous marketing campaigns:\n\\[Install_{Ref} =  User_{Limit}*CDF_{log-norm}(Cost_{user, reference}, \\mu, \\sigma)\\]\n\\[User_{Limit} = \\frac{Install_{Ref}}{CDF_{log-norm}(Cost_{user, reference}, \\mu, \\sigma)}\\]\nWe will show for different log-normal distributions how the volume should vary per \\(Cost_{user}\\), and, consequently, the profit curve and the optimal bidding point. For that, we fix the following variables.\n\\[Cost_{user, reference} = 2\\] \\[E[LTV] = 2\\] \\[Install_{Ref} = 1000\\]\nIn addition, we will consider four different log-normal distributions for the Volume Function, with expected values of 0.5, 1.0, 1.5, and 2 and a standard deviation of 0.5 (for the normal distribution).\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom joblib import Parallel, delayed\nfrom typing import List\n\nfrom src.VolumeAcquisition import CumulativeLognormalVolume\n\nsns.set_style('whitegrid')\nrng = np.random.default_rng(42)\nPLOT_WITHD_INCHES = 20\nPLOT_HEIGHT_INCHES = 10\n# The fixed parameters specified before\nreference_volume = 1000\nreference_cpi = 2.0\nreference_ltv = 2.0\n\nlognormal_exp_values = [0.5, 1, 1.5, 2.0] # the expected values for the \ncpi_range = np.linspace(0.001, 6.5, 1000) # we want to consider cost per user between $0 and $6.5\ndef generate_marketing_properties(\n    cpi_range: list, \n    lognormal_expected_values: List[float],\n    reference_volume: int,\n    reference_cpi: float,\n    reference_ltv: float\n):\n    \"\"\"\n    This class calculates (volume, revenue, profit, cost) for each CPI value in [cpi_range].\n    The volume curve changes depending on the value from [volume_params], but it always gives [reference_volume] for [reference_cpi]\n    In addition to the marketing properties, it also provides estimates on the CDF and PDF of the assumed distribution of volume by CPI\n    \"\"\"\n    output_data = []\n    for exp_value in lognormal_expected_values:\n        # define how the volume behaves\n        volume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, average=exp_value, standard_deviation=0.5)\n        \n        # calculate volume, and cdf for diferent cpis\n        volume = volume_model.calculate_volume(cpi_range)\n        cdf = volume_model.calculate_cdf(cpi_range)\n        pdf = volume_model.calculate_pdf(cpi_range)\n        df = pd.DataFrame({'cpi': cpi_range, 'volume': volume, 'cdf': cdf, 'pdf': pdf})\n        \n        # store which lognormal expected value was used\n        df['expected_value'] = exp_value\n        output_data.append(df)\n\n    # merge the data from the different volume functions together\n    output_data = pd.concat(output_data)\n    \n    # calculate the profit for each cpi\n    output_data['profit'] = output_data['volume'] * (reference_ltv - output_data['cpi'])\n\n    # Cast the avg to make the seaborn interpret the values as category\n    output_data['expected_value'] = output_data['expected_value'].astype(str)\n    return output_data\n# generate the volume and profit curves for our scenarios and store them in a pd.DataFrame\nlog_norm_volume_data = generate_marketing_properties(\n    cpi_range,\n    lognormal_exp_values,\n    reference_volume,\n    reference_cpi,\n    reference_ltv\n    \n)\nBelow you can see how the density probability functions with the selected parameters looks like\ngrid = sns.relplot(log_norm_volume_data, x='cpi', y='pdf', hue='expected_value', kind='line')\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Probability density function (Y) versus cost per user (X), versus mean of the log-normal distribution', \n    font='Avenir', \n    fontsize=24, \n    loc='left'\n)\nText(0.0, 1.0, 'Probability density function (Y) versus cost per user (X), versus mean of the log-normal distribution')\n\nlog_norm_volume_data = log_norm_volume_data[log_norm_volume_data['cpi']< 2.5]\ngrid = sns.relplot(log_norm_volume_data, x='cpi', y='volume', hue='expected_value', kind='line')\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Number of installs (Y) versus cost per install (X), versus mean of the log-normal distribution', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Number of installs (Y) versus cost per install (X), versus mean of the log-normal distribution')\n\nEach curve can represent different advertisement platforms: the blue curve could be a small platform where most users can already be acquired with a relatively low cost of $4, while the red curve can represent a platform with many users and a lot of competition from other advertisers since there are a lot of users we can acquire - as the curve quickly increases after $2."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-profit-curve-how-the-expected-profit-varies-per-cost-per-user",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-profit-curve-how-the-expected-profit-varies-per-cost-per-user",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "3- The Profit Curve: how the expected profit varies per Cost per User",
    "text": "3- The Profit Curve: how the expected profit varies per Cost per User\nBy using these curves in the \\(Profit\\) function, we get the following profit curve by cost per user.\ngrid = sns.relplot(log_norm_volume_data, x='cpi', y='profit', hue='expected_value', kind='line')\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Predicted expected profit (Y) versus cost per install (X), versus mean of the log-normal distribution', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Predicted expected profit (Y) versus cost per install (X), versus mean of the log-normal distribution')\n\nWhich all have slightly different optimal \\(Cost_{user}\\)\nlog_norm_volume_data.sort_values(['expected_value', 'profit']).groupby(['expected_value'])['cpi'].last().reset_index()\n\n\n\n\n\n\n\n\n\nexpected_value\n\n\ncpi\n\n\n\n\n\n\n0\n\n\n0.5\n\n\n1.360651\n\n\n\n\n1\n\n\n1.0\n\n\n1.536299\n\n\n\n\n2\n\n\n1.5\n\n\n1.653398\n\n\n\n\n3\n\n\n2.0\n\n\n1.731464\n\n\n\n\n\n\nWhile the highest profit isn’t relevant when comparing the curves, since we forced all the curves to provide the same number of users at $2, the ‘shape’ is. Notice how the profit curve drawn above is asymmetric around the optimal \\(Cost_{user}\\) for most curves, as the profit quickly drops as the \\(Cost_{user}\\) increases from its optimal point. This asymmetry will be essential to understand why and when the uncertainty on LTV matters."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-error-associated-with-the-predicted-ltv",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#the-error-associated-with-the-predicted-ltv",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "4- The error associated with the predicted LTV",
    "text": "4- The error associated with the predicted LTV\nWith the clear connection between Cost per User and profit, we can focus on LTV. The Lifetime Value of a user is, by definition, how much revenue a user generates in their lifetime, in other words, from the moment they started their ‘life’ as a user of a product until infinity. But we can’t use infinity in practice, in good part because we need a target to train the machine-learning models, so the lifetime value is usually defined as just a date ‘sufficiently away’ in the future.\nThe fact that the ‘real’ LTV is usually too far away in the future means that it cannot be used for practically any critical decision within a company. This means that the LTV used for such decisions will be an estimate, and as an estimate, we are never sure of its exact value.\nIf we estimate the LTV of our users using any conventional machine-learning algorithm, one assumption that these models require is for the residual (basically the error) to be normally distributed. As such, we will assume that the LTV models’ error is (1) unbiased and (2) follow a normal distribution.\n\\[LTV \\sim N(\\mu, \\sigma) \\]\n\\[LTV_{predicted} = LTV + \\epsilon\\]\n\\[\\epsilon \\sim N(0, \\delta)\\]\n, where \\(\\epsilon\\) is the residual and \\(\\delta\\) is its standard devitiation.\nrng = np.random.default_rng(42)\nltv_error_std = 0.5\nnorm_dist = rng.normal(0, ltv_error_std, 10000)\n\ngrid = sns.histplot(norm_dist)\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Distribution of the error of the LTV predictions', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Distribution of the error of the LTV predictions')\n\nAgain, I will reinforce that LTV is a predicted metric, and as such until proven otherwise, it is what we believe to be true. That is the case for any metric, but for most metrics, ML models predict, it becomes clear relatively fast if the model was wrong or not. But it can take many months until signs appear of bias in the LTV predictions, which can mean several thousand, if not tens or hundreds of thousands of dollars, invested in marketing that are not coming back\nFor example, assume five different scenarios where we keep the actual LTV at 2 dollars and use the Volume Function in orange from the first plot but have the estimated LTV to be $1, $1.5, $2, $2.5, or  $3. If we know the Volume Function and we were to optimize for the predicted expected value of LTV without budget being a constrain, we would operate at the indicated estimated optimal cpi, believe we would obtain the estimated profit while in practice it would be the real profit\npossible_ltvs = [1, 1.5, 2, 2.5, 3, 3.5]\nfixed_cpi = [1]\nestimated_optimal_cpi = []\nfor ltv in possible_ltvs:\n    marketing_data = generate_marketing_properties(\n        cpi_range,\n        fixed_cpi,\n        reference_volume,\n        reference_cpi,\n        ltv\n    )\n    estimated_optimal_cpi.append(list(marketing_data.sort_values(['profit'])['cpi'])[-1])\n\nestimated_optimal_cpi = pd.DataFrame({'cpi': estimated_optimal_cpi, 'estimated_ltv': possible_ltvs})\n\nestimated_optimal_cpi = pd.merge(\n    log_norm_volume_data[log_norm_volume_data['expected_value'] == '1.0'],\n    estimated_optimal_cpi,\n    on='cpi'\n)\nestimated_optimal_cpi['estimated profit'] = (estimated_optimal_cpi['estimated_ltv'] - estimated_optimal_cpi['cpi'])*estimated_optimal_cpi['volume']\nestimated_optimal_cpi['real profit'] = estimated_optimal_cpi['profit']\n\n\n\nestimated_ltv\ncpi\nvolume\nestimated profit\nreal profit\n\n\n\n\n1.0\n0.840210\n34.971618\n5.588107\n40.559725\n\n\n1.5\n1.204519\n191.985179\n56.728065\n152.720654\n\n\n2.0\n1.536299\n470.453096\n218.149430\n218.149430\n\n\n2.5\n1.829047\n793.685368\n532.525541\n135.682857\n\n\n3.0\n2.095773\n1117.805605\n1010.750263\n-107.055342\n\n\n3.5\n2.336476\n1412.857895\n1643.893396\n-475.393446\n\n\n\nNotice the discrepancy between the estimated and actual profits when we overestimate LTV. While this discrepancy is minor when the predicted LTV is underestimated (and usually, getting more money than expected isn’t badly received news), the disparity quickly increases when the predictions are overestimated.\nThis behavior can be better understood when we plot the profit distribution when the standard deviation is 1 (i.e., 50% of the actual LTV). As shown below, while the profit is mostly positive, we have a situation where it is negative, as demonstrated by the long left tail.\n# See how the distribution of the profit is, when we are wrong about the LTV\nfrom src.ProfitModel import StandardModel\nvolume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, average=1, standard_deviation=0.5)\nprofit_model = StandardModel(volume_model, lifetime_value=2)\n\n# calculate the profit\ncpis = rng.normal(reference_ltv, reference_ltv*ltv_error_std, 10000)\nprofit_distribution = profit_model.calculate_profit(cpis)\n\ngrid = sns.histplot(profit_distribution)\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    f'Distribution of the profit when the LTV error is normally distributed with average 0 and standard deviation equal to {ltv_error_std*reference_ltv}', \n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Distribution of the profit when the LTV error is normally distributed with average 0 and standard deviation equal to 1.0')\n\nWhile one may think that a 50% error for a prediction is too high, that is quite common for a new product being launched, for a marketing platform being introduced, or for a novel optimization within an existing platform. I.E. for any case where there may not be much data available and the situation is sufficiently novel from what was seen before. So given the risk involved in incorrectly predicting and specially overestimating the LTV, how should we adapt the bidding strategy change for different degrees of uncertainty?"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#step-by-step-explanation-of-the-simulation",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#step-by-step-explanation-of-the-simulation",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "5.1- Step-by-step explanation of the simulation",
    "text": "5.1- Step-by-step explanation of the simulation\nWe set up the parameters for the simulation\n\nsample_size defines how many simulations we are doing. In the code it is called sample size because, effectively, each simulation is sampling a value for the LTV error from a normal distribution. In this step we are going to do 1 sample.\nstandard_deviation_of_ltv_error defines the range considered for the simulation for the standard error of LTV. This value is relative to the expected LTV.\nfraction_of_estimated_ltv defines the fraction of the predicted LTV we use to bid. So if we predicted a LTV of $1.0 and the selected fraction is 0.8, we will re-scale the LTV to be $0.80 and find the optimal Cost per User under this scenario. While we state it is a fraction, we include values greater than 1 just to demonstrate that it is never better to ‘overpay’ for the Volume Functions used throughout this document\n\nfrom src.BidOptimizer import StandardBidOptimizer\n\nsample_size = 1\nstandard_deviation_of_ltv_error = [0.4]\nfraction_of_estimated_ltv = [0.6, 0.8]\navg = 2\nWe then define the volume model with the previously mentioned parameters:\nvolume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, avg, .5)\nAnd now we create the bid optimizer object bid_optim. This class has 2 main methods:\n\nsimulate(): This method is going to extract samples from the normal distribution, then for each of the selected fractions we multiply the predicted LTV to obtain the adjusted LTV, which then is used to find the optimal Cost per User. Finally, this Cost per User is used to calculate the profit.\ncalculate_bidding_strategy_results(): This method uses the previous output, and calculates for each LTV standard error and fraction, the average profit. Then, for each standard error, it finds which LTV fraction provided the highest average profit\n\nbid_optim = StandardBidOptimizer(\n    volume_model, \n    standard_deviation_of_ltv_error, \n    fraction_of_estimated_ltv, \n    reference_ltv_value=2.0,\n    sample_size=sample_size)\n# simulate the 2 scenarions (because we set sample_size to 2)\nsim_results = bid_optim.simulate()\n\n# see the output from the simulation:\nsim_results\n\n\n\n\n\n\n\n\n\nestimated_ltv\n\n\nltv_fraction\n\n\ncpi\n\n\nprofit\n\n\nsd\n\n\n\n\n\n\n0\n\n\n2.121887\n\n\n0.6\n\n\n1.131382\n\n\n16.937414\n\n\n0.4\n\n\n\n\n1\n\n\n2.121887\n\n\n0.8\n\n\n1.484203\n\n\n76.374987\n\n\n0.4\n\n\n\n\n2\n\n\n1.584006\n\n\n0.6\n\n\n0.855907\n\n\n2.074194\n\n\n0.4\n\n\n\n\n3\n\n\n1.584006\n\n\n0.8\n\n\n1.126308\n\n\n16.435281\n\n\n0.4\n\n\n\n\n\n\nYou can see that we have 2 rows for each estimated_ltv as a consequence of the 2 different ltv_fraction selected, resulting in the predicted optimal Cost per User cpi. This cpi is then used to acquire users and results in the actual profit you see there. Remeber that for all simulations the actual LTV is held at $2.0\nWe now calculate which of the LTV fractions gave the highest average profit:\nbid_optim.calculate_bidding_strategy_results(sim_results)\nbid_optim.bidding_strategy_data\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n\n\n\n\n\n\n\n\n\nsd\n\n\nmean_profit\n\n\nstd_profit\n\n\nltv_fraction\n\n\n\n\n\n\n0\n\n\n0.4\n\n\n46.405134\n\n\n29.969853\n\n\n0.8\n\n\n\n\n\n\nWhich in this case was 0.8, at $72.36"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#simulation",
    "href": "posts/Forecasting Customer Lifetime Value - Why Uncertainty Matters/index.html#simulation",
    "title": "Forecasting Customer Lifetime Value - Why Uncertainty Matters",
    "section": "5.2- Simulation",
    "text": "5.2- Simulation\nNow that it is clear what is happening, we can find for each of the Volume Functions from before and for different levels of LTV standard error, which fraction gives the highest average profit\nsample_size = 5000\nstandard_deviation_of_ltv_error = np.linspace(0.001, 0.4, 10)\nfraction_of_estimated_ltv = np.linspace(0.3, 1.2, 20)\noutput_data = []\n# for each distribution of the Volume Function\nfor avg in lognormal_exp_values:\n    # set up the volume model\n    volume_model = CumulativeLognormalVolume(reference_volume, reference_cpi, avg, .5)\n    \n    # set up the class that will sample the standard error, and calculate the resulting bid and profit\n    bid_optim = StandardBidOptimizer(\n        volume_model, \n        standard_deviation_of_ltv_error, \n        fraction_of_estimated_ltv, \n        sample_size=sample_size)\n    \n    # run the N=sample_size simulations\n    sim_results = bid_optim.simulate()\n    \n    # calculate which LTV fraction gave the highest average profit for each LTV standard error level\n    bid_optim.calculate_bidding_strategy_results(sim_results)\n    \n    # store the results\n    bid_optim.bidding_strategy_data['lognormal_avg'] = avg\n    output_data.append(bid_optim.bidding_strategy_data)\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\n/Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting/src/BidOptimizer.py:80: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  self.bidding_strategy_data = self.bidding_strategy_data.groupby('sd')['mean_profit', 'std_profit', 'ltv_fraction'].first().reset_index()\nplot_data = pd.concat(output_data)\nplot_data['lognormal_avg'] = plot_data['lognormal_avg'].astype(str)\n\ngrid = sns.relplot(plot_data, \n                   x='sd', \n                   y='ltv_fraction', \n                   hue='lognormal_avg', \n                   kind='line', \n                   facet_kws={'ylim': [0, 1.1]}\n                  )\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Fraction of Cost per User with highest average profit when there is not uncertainty (Y) versus error level of LTV (X), versus mean for log-normal distribution',\n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Fraction of Cost per User with highest average profit when there is not uncertainty (Y) versus error level of LTV (X), versus mean for log-normal distribution')\n\ngrid = sns.relplot(plot_data, \n                   x='sd', \n                   y='mean_profit', \n                   hue='lognormal_avg', \n                   kind='line',\n                   facet_kws={'ylim': [0, 400]}\n                  )\ngrid.figure.set_size_inches(PLOT_WITHD_INCHES, PLOT_HEIGHT_INCHES)\nplt.title(\n    'Average profit from the optimal LTV fraction strategy (Y) versus error level of LTV (X), versus mean for log-normal distribution',\n    font='Avenir', \n    fontsize=24, \n    loc='left')\nText(0.0, 1.0, 'Average profit from the optimal LTV fraction strategy (Y) versus error level of LTV (X), versus mean for log-normal distribution')\n\nAs expected, the greater the uncertainty of LTV, the more ‘conservative’ we should be on our marketing strategy. But that doesn’t only depend on the LTV uncertainty itself but also on the Volume Function. For Volume Functions where the profit curve is highly asymmetric around the optimal Cost per User, such as when \\(lognormal_{avg}\\) is 2 (red), even for a slight standard error of 0.1 we already have to bid less than the expected optimal point. However, we don’t need to be as conservative when the Profit Curve is more symmetric (i.e. the volume doesn’t increase as fast).\nNotice also how the profit curve decreases similarly for all Volume Functions from where LTV error is close to 0 to where it is close to 0.4. However, the relative profit loss is much higher when the volume increase fast. While the loss for \\(lognormal_{avg}\\) equal to 0.5 (blue) from the two extremes is less than 9%, for \\(lognormal_{avg}\\) equal to 2 (red) the loss is 48%. That is a result from the need to bid further away from the optimal point and from the higher losses, when we overbid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Raphael Tamaki's Blog",
    "section": "",
    "text": "lifetime value\n\n\nmarketing\n\n\ndigital marketing\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nRaphael de Brito Tamaki\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlifetime value\n\n\nmarketing\n\n\ndigital marketing\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nRaphael de Brito Tamaki\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my personal blog, where I share some thoughts in topics I have experience or interest in. Feel free to reach out to me in LinkedIn or on my email"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Challenges/index.html",
    "href": "posts/Forecasting Customer Lifetime Value - Challenges/index.html",
    "title": "Forecasting Customer Lifetime Value - Challenges",
    "section": "",
    "text": "What are common challenges that we face when building LTV predictions?"
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Challenges/index.html#one-model-to-rule-them-all",
    "href": "posts/Forecasting Customer Lifetime Value - Challenges/index.html#one-model-to-rule-them-all",
    "title": "Forecasting Customer Lifetime Value - Challenges",
    "section": "1- One model to rule them all?",
    "text": "1- One model to rule them all?\nOne of the most significant benefits of working with LTV estimation is the vast impact that it can have across the company. After all, the Lifetime Value can be used for different goals, such as:\n\nprioritizing which product a company should focus on: the greater the (potential) profit a product has, the greater focus the company should have on it\nforecasings the cash flow of the following months, thus predicting the financial health of the company\ndirectly influencing marketing by estimating the profitability of endeavors of the marketing team. With an accurate Lifetime Value estimation, the marketing team can quickly scale winning strategies and cut back on the approaches that are not worth it.\n\nBut this broad impact of Lifetime Value estimation is a double-edged sword because each of those areas has its own needs and definition of an optimum model, which means that a change in the model can have opposite impact on distinct areas. One example is a change that worsens predictions for older users but improves predictions for newer users. This change is positive for the Marketing Team since they mostly care about the prediction for recent users, who were likely acquired by a marketing campaign so that they can better adjust their campaigns bid. On the other hand, the Product Team will oppose such change since older users form the most extensive user base of a product, and worsening the LTV on them may mean applying product changes that are detrimental to the existing users.\nThis challenge has been previously explored in Pitfalls of Modeling LTV and How to Overcome Them from Algolift (now Liftoff). There it brings four dimensions that may be different depending on the use cases: Stability, Accuracy, Temporal granularity, and Cohort granularity:\n\nStability: What is a reasonable forecast update frequency? Frequent changes indicates inaccuracy. However, ignoring real surprising behavior that strongly changes LTV is also undesirable.”\nAccuracy: What are an acceptable level of variance (sample size) and bias (model sophistication)?\nTemporal granularity: How soon after installation does the forecast come? Is a cohort a week or a month? Different teams care about different temporal granularity and responsiveness.\nCohort granularity: Does your end-user care about individual users, all users, or a country?\n\nThen, Algolift shows how each use case has different needs and interests for the LTV predictions on those four dimensions:\n\n\n\n\n\n\n\n\n\n\nApplication\nStability\nAccuracy\nTemporal Granularity\nCohort Granularity\n\n\n\n\nMarketing Automation\nLess important than responsiveness to market dynamics\nImportant to be directionally correct without channel/geo bias\n0 to 14 days\nCampaign: 100 to 1000 users. Paid traffic\n\n\nAccounting and Global Revenue Forecasting\nMust be stable\nVery important for forecast accuracy\nQuarterly or Yearly\nCountry, platform, network, Paid and Organic\n\n\nProduct\nMust be stable\nRelative change more important than absolute value\nMonths\nCountry, Platform, network, Paid and Organic\n\n\n\nThis table shows how each use case has different requirements for the LTV predictions. For example, while Marketing Automation requires that the projections are frequently updated to respond to market changes, this would be detrimental to Accounting. When Accounting needs to forecast the company’s global revenue for the following quarter (or year), having the LTV predictions change means that they can’t validate their forecasts.\n\n1.1- Divide and conquer\nSince each use case has its demands, creating one model for each use case appears as the most sensible answer. With each model optimized for only one specific metric, it is almost certain that we will reach a better value on each optimization metric. In addition, iterations on the model are much faster by having only one metric to optimize for one stakeholder, which again contributes to a better optimization metric.\nBut this approach has a scalability problem caused by the maintenance cost for the models and the need to solve for inconsistencies when each model accuses a different LTV. The differences in the predicted LTV are because each model is optimized for other goals, so they won’t necessarily indicate the same absolute values or relative changes in the product or the company. Thus we end up with another much more critical inconsistency problem: the perceived impact of strategic decisions. How can teams conclude when decisive information points to different directions, depending on which team sees it?\nThe answer is figuring out the source of divergence. While the models may output different values since their targets are not precisely the same, they should be reasonably similar. After all, the target is still the same (LTV), and the cost function is usually the same (ex: MSE). If we notice that models indicate values outside of the typical divergence, we know that at least one of them needs attention. This divergence can be calculated by comparing the output of the models on a period where it is known to have no deviations and then calculating a metric (ex: Mean Squared Error). Anytime the divergence metric falls outside the accepted value, an investigation must be conducted.\nWhile the solution is simple, understanding the reason for the diverging predictions is time-consuming. It means that a Data Scientist, Data Analyst, or Machine Learning Engineer/Scientist has to stop their work to figure out the source of the problem. Depending on the complexity of the problem, it may take several days to find the cause of the differences. Even worse is when the source is not found even after lengthy investigations, and the problem just gets dropped because it is no longer relevant or considered part of the usual divergences between models.\nThe second problem is the cost of maintaining all the models within acceptable performance standards. This cost can quickly increase as different models often mean different codebases, so the responsible team needs to update in multiple places the same changes or be aware of all the minor differences between the models. As the number of models increases with complexity, supporting various models becomes unsustainable\nThis problem becomes even clearer when we remember that the machine-learning code is just a small component of the machine-learning system, as best explained in Hidden Technical Debt in Machine Learning Systems. As shown in the article through the image below, the serving system required to deliver machine-learning predictions has many other components besides the machine-learning (ML) code. And these other components often contain larger codebases\n\n\n\nDepiction of the surrounding infrastructure surrounding the ML code\n\n\nBy having multiple-machine learning code, we are also multiplying the required infrastructure surrounding our code.\n\n\n1.2 - My stance: one model to rule them all\nWith the issues raised before on having multiple LTV models, the best solution appears to always have a single one. Sadly, there is no universal solution for these practical problems. Whether building additional models is beneficial for the company depends on the available headcount to create and maintain these models, the financial impact that more accurate LTV predictions have, and the complexity of the existing code-base.\nBut to take a stance on this topic, most companies should benefit from having a single machine-learning code-base for the LTV. Based the 4 dimensions brought by AlgoLift and an additional we touched before, I argue that one model (or at least one code base) is frequently the best solution:\n\nTemporal and Cohort Granularity (AlgoLift): The granularity on how LTV predictions are consumed can be easily accommodated by adding a step that adjusts depending on the need. When the LTV Model predictions are more granular or as granular than what any stakeholder requires, a grouping step\nStability (AlgoLift): as shown before, Product and Accounting may require that predictions are constant through time: once something is predicted, it doesn’t change any more. Keeping projections constant can also be solved by creating a mutable and immutable version of LTV predictions.\nAccuracy (AlgoLift): While clients can have different goals for the LTV model to optimize, most often, they will not cause divergence. For example, the Product cares for directional changes and Marketing Automation for the absolute values. If the LTV model doesn’t follow the changes, it will be biased and won’t satisfy the requirement for marketing.\nIteration speed: the most potent argument for having multiple models is the ability to implement changes quickly. When a company has numerous different products with unique user bases and particularities, having more than one model can be better. Still, the various models should share many similarities, and I’m in favor of sharing as much code as possible between the models and periodically attempting to unify them. Quite often, the solutions particular to one case are because we need to understand the underlying mechanism properly. When we do, we can find a solution that i s beneficial for all use cases.\n\n\n\n1.3- Conclusion\nPredicting Lifetime Value can broadly impact the company, creating demands for different variations optimized for each use case. While there are cases where a company should have multiple LTV models, I defend holding with a single model (or at least a single code-base) as much as possible."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Challenges/index.html#should-you-be-using-a-supervised-approach-at-all",
    "href": "posts/Forecasting Customer Lifetime Value - Challenges/index.html#should-you-be-using-a-supervised-approach-at-all",
    "title": "Forecasting Customer Lifetime Value - Challenges",
    "section": "2- Should you be using a supervised approach at all?",
    "text": "2- Should you be using a supervised approach at all?\n\n\n\nVicious cyle of over- and underestimation of campaigns by LTV models\n\n\nIn Lifetime Value prediction, the most common approach is using supervised machine-learning algorithms such as XGBoost. After all, this problem can be easily organized as tabular data, as in the Lifetime Value dataset from Kaggle:\n!kaggle datasets download -d baetulo/lifetime-value\n!unzip -j lifetime-value.zip \n!rm lifetime-value.zip test.csv\nDownloading lifetime-value.zip to /Users/raphaeltamaki/Documents/personal_git/lifetime_value_forecasting\n 95%|████████████████████████████████████  | 10.0M/10.5M [00:00<00:00, 49.0MB/s]\n100%|██████████████████████████████████████| 10.5M/10.5M [00:00<00:00, 45.9MB/s]\nArchive:  lifetime-value.zip\n  inflating: test.csv                \n  inflating: train.csv               \nimport pandas as pd\npd.read_csv('train.csv').head()\n\n\n\n\n\n\n\n\n\nproduct_type\n\n\nuser_id\n\n\njoin_date\n\n\nhidden\n\n\nproduct\n\n\nSTV\n\n\ntarget\n\n\ncredit_card_level\n\n\nis_lp\n\n\naff_type\n\n\nis_cancelled\n\n\ncountry_segment\n\n\n\n\n\n\n0\n\n\ntype_ex\n\n\n7.0\n\n\n2018-12-01 00:01:45\n\n\n0\n\n\nproduct_1\n\n\n8.25\n\n\n8.25\n\n\nstandard\n\n\n0\n\n\nPPL\n\n\nNaN\n\n\nUS\n\n\n\n\n1\n\n\ntype_ex\n\n\n20.0\n\n\n2018-12-01 00:06:05\n\n\n0\n\n\nproduct_2\n\n\n8.25\n\n\n8.25\n\n\nstandard\n\n\n0\n\n\nPPL\n\n\nNaN\n\n\nUS\n\n\n\n\n2\n\n\ntype_ex\n\n\n22.0\n\n\n2018-12-01 00:06:23\n\n\n0\n\n\nproduct_3\n\n\n8.25\n\n\n8.25\n\n\nprepaid\n\n\n0\n\n\nPPL\n\n\nNaN\n\n\nUS\n\n\n\n\n3\n\n\ntype_ex\n\n\n26.0\n\n\n2018-12-01 00:07:12\n\n\n0\n\n\nproduct_2\n\n\n8.25\n\n\n8.25\n\n\nstandard\n\n\n0\n\n\nPPL\n\n\nNaN\n\n\nUS\n\n\n\n\n4\n\n\ntype_ex\n\n\n59.0\n\n\n2018-12-01 00:15:21\n\n\n0\n\n\nproduct_2\n\n\n8.25\n\n\n8.25\n\n\nstandard\n\n\n0\n\n\nPPL\n\n\nNaN\n\n\nOther Countries\n\n\n\n\n\n\nWhile commonplace, this approach faces a problem when its LTV predictions define bids (i.e., CPI, CPA, ROAS) in marketing campaigns. When they are, we create a feedback loop in the system, where the model’s predictions for a set of users impact the probability of further having similar users. Take the example below, where we have 2 marketing campaigns with slightly different types of users. And because they are other users, our model outputs somewhat different predictions, even though they have the same true LTV of $2.00.\nTake the example below, where we have 2 marketing campaigns with slightly different types of users. And because they are other users, our model outputs somewhat different predictions, even though they have the same true LTV of $2.00.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nTrue LTV\nPredicted LTV (A)\nPredicted LTV (B)\nCPI (A)\nCPI (B)\nUsers (A)\nUsers (B)\nTrue Revenue (A+B)\nPredicted Revenue (A+B)\nBias\nCost\nProfit\n\n\n\n\nPeriod 1\n$2\n$3\n$1\n$1\n$1\n100\n100\n$400\n$400\n0%\n$200\n$200\n\n\n\nBased on the LTV predictions, the company adjusts the bids for the 2 campaigns by increasing Campaign A’s CPI and decreasing Campaign B’s. Let’s assume for simplicity that the number of users acquired by the 2 campaigns is proportional to their CPI, thus following a linear relationship. In this case, the optimal CPI equals half of the LTV. We then use the updated bids to acquire some users until we observe their LTV.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nTrue LTV\nPredicted LTV (A)\nPredicted LTV (B)\nCPI (A)\nCPI (B)\nUsers (A)\nUsers (B)\nTrue Revenue (A+B)\nPredicted Revenue (A+B)\nBias\nCost\nProfit\n\n\n\n\nPeriod 1\n$2\n$3\n$1\n$1\n$1\n100\n100\n$400\n$400\n0%\n$200\n$200\n\n\nPeriod 2\n$2\n$3\n$1\n$1.50\n$0.50\n150\n50\n$400\n$500\n25%\n$250.00\n$150\n\n\n\nWe see that even though initially the LTV model was not biased, we ended up with a biased model after the model’s inputs were used to adjust the bids in the marketing campaigns. In addition, notice that this is not the problem of drift common when training the model since the correct value of LTV didn’t change. And to make things worse, we often only realize that the model is biased because the target (i.e., LTV) of the recently acquired users is only observed after a long time.\nHowever, this is not a problem that always occurs. It only happens when the bids for the campaigns need to be adjusted with the predicted LTV. When they don’t need to be adjusted, we don’t have the previous problem since the number of users doesn’t change.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nTrue LTV\nPredicted LTV (A)\nPredicted LTV (B)\nCPI (A)\nCPI (B)\nUsers (A)\nUsers (B)\nTrue Revenue (A+B)\nPredicted Revenue (A+B)\nBias\nCost\nProfit\n\n\n\n\nPeriod 1\n2\n$3\n$1\n$1.50\n$0.50\n100\n100\n$400\n$400\n0%\n$200.00\n$200\n\n\nPeriod 2\n2\n$3\n$1\n$1.50\n$0.50\n100\n100\n$400\n$400\n0%\n$200.00\n$200\n\n\n\nHowever, this is an unlikely situation in practice, especially for digital products. The LTV of the recent users varies a lot and quite often, be it because of internal product changes (ex: promotions and feature releases) or because of macro influences (ex: summer vacation). Consequently, the bid used in a marketing campaign needs to be constantly adjusted, and the problem shown before appears.\n\n2.1- Correcting the LTV Model\nLet’s go back to the first case and suppose that we notice the bias after some time and then retrain the model using the date on the most recent users to remove part of it. We then use the new LTV predictions to adjust the bids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nTrue LTV\nPredicted LTV (A)\nPredicted LTV (B)\nCPI (A)\nCPI (B)\nUsers (A)\nUsers (B)\nTrue Revenue (A+B)\nPredicted Revenue (A+B)\nBias\nCost\nProfit\n\n\n\n\nPeriod 2\n2\n$2.50\n$1.50\n$1.50\n$0.50\n150\n50\n$400\n$450\n13%\n$250.00\n$150\n\n\nPeriod 3\n2\n$2.50\n$1.50\n$1.25\n$0.75\n125\n75\n$400\n$425\n6%\n$212.50\n$188\n\n\n\nIn this situation, the retraining of the model helped make it less biased and increase profits. But notice that we didn’t negate the bias. We only decreased it. What if we tried to zero the bias? Unless we perfectly predicted each audience - which is extremely unlikely, and if achieved, is most likely due to overfitting - then you may have ‘overcorrected’ it:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nTrue LTV\nPredicted LTV (A)\nPredicted LTV (B)\nCPI (A)\nCPI (B)\nUsers (A)\nUsers (B)\nTrue Revenue (A+B)\nPredicted Revenue (A+B)\nBias\nCost\nProfit\n\n\n\n\nPeriod 2\n2\n$1.50\n$3.50\n$1.50\n$0.50\n150\n50\n$400\n$400\n0%\n$250.00\n$150\n\n\nPeriod 3\n2\n$1.50\n$3.50\n$0.75\n$1.75\n75\n175\n$500\n$725\n45%\n$362.50\n$138\n\n\n\nWhen the errors on those campaigns are large, this can create a vicious cycle where campaigns are scaled up and down as they get over- and undervalued. This happens because to remove the bias, the only way besides nailing the values is by inverting which segment is over- /underestimated. This is caused by the disbalance between the number of users suffering from over-/underestimation, requiring you to overestimate the (now) small population to compensate for the larger population of the (then) overestimated users.\nNotice further that this solution is not solved by considering the variance of the error, for example, by using Mean Squared Error. The change shown in the table above would still decrease the MSE , measured by the predicted and true revenue of each campaign, for the first period even if later it causes the same problem:\n\\[MSE = \\sum\\left(\\left(Revenue_{predicted} - Revenue_{Observed}\\right)^2\\right)\\]\n\\[MSE_{period\\ 2,\\ initial predictions} =\\frac{\\left(150*3 - 150*2\\right)^2 + \\left(50*1 - 50*2\\right)^2}{2} = 12500\\]\n\\[MSE_{period\\ 2,\\ new predictions} = \\frac{(\\left(150*1.5 - 150*2\\right)^2 + \\left(50*3.5 - 50*2\\right)^2}{2} = 5625\\]\nSo while it may be initially unintuitive, having your models constantly updated on the most recent data and aiming for no bias may keep the bias present in the model, just alternating the contributors to the bias.\n\n\n2.2- Solving the ‘overcorrection’\nSince the predictions of the model influence the data that will appear in the future, this sounds like a typical use case for Reinforcement Learning. While it is true that the model is affecting its future data, this doesn’t fall into the domain of Reinforcement Learning because the action (i.e., the bids used to acquire the users) doesn’t influence the users themselves. The bid only changes the proportion of the type of users that the model sees.\nThe immediate thought is that we should thus ignore weighting the campaigns by how much revenue they generate. While this will remove the bias, it is a problem of neglecting the most relevant campaigns and overvaluing the small ones. This solution is especially problematic for a company with a mature marketing department, where they constantly explore marketing strategies through small experiments.\nA better solution is to keep the use-base proportion constant. Instead of directly using the data of the users of the most recent period, sample them based on the number of users from before and use the sampled data to train the LTV models. With this, we still use the most recent (and thus informative) data about our users while avoiding the problem we just highlighted.\nTake the example from before where we ‘overcorrected’ for the bias. In that case, not only did we remove the bias, but we also decreased the variance (as per the MSE). If we were instead to use the number of users (or proportion of users) from the previous period, we would see both an actual increase in bias and variance:\n\\[MSE_{period\\ 1,\\ initial predictions} =\\frac{\\left(100*3 - 100*2\\right)^2 + \\left(100*1 - 100*2\\right)^2}{2} = 10000\\]\n\\[MSE_{period\\ 1,\\ new predictions} = \\frac{(\\left(100*1.5 - 100*2\\right)^2 + \\left(100*3.5 - 100*2\\right)^2}{2} = 12500\\]\n\n\n2.3- Conclusion\nLifetime Value estimation is already a complex problem, but it can be worse when the model’s predictions are used in marketing campaigns. In this case, using standard metrics such as MSE will not prevent the LTV models from causing a vicious cycle of over- and underestimation of different segments of users. And while it may seem that a Reinforcement Learning approach could solve the problem, it won’t. Instead, one should still use a supervised learning approach but not directly use the most recent users to train the models and sample the new users based on the proportion of users from the same segment in a previous period."
  },
  {
    "objectID": "posts/Forecasting Customer Lifetime Value - Challenges/index.html#references",
    "href": "posts/Forecasting Customer Lifetime Value - Challenges/index.html#references",
    "title": "Forecasting Customer Lifetime Value - Challenges",
    "section": "References",
    "text": "References\n\nWhy Uncertainty Matters\nPitfalls of Modeling LTV and How to Overcome Them\nHidden Technical Debt in Machine Learning Systems"
  }
]